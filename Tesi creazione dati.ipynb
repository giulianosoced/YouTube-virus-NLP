{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "from networkx.algorithms import bipartite\n",
    "import nltk\n",
    "from nltk import sent_tokenize, wordnet, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import conll2000, stopwords\n",
    "from pandas import HDFStore, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN THE ORIGINAL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read the comment file. maybe change the encoding..\n",
    "comments=pd.read_csv('commenti.csv', encoding = \"ISO-8859-1\")\n",
    "comments=comments.drop('Unnamed: 0', axis=1)\n",
    "comments=comments.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop parent_id, moderation status and drop NAs\n",
    "comments=comments.drop(['parent_id', 'moderationStatus', \n",
    "                        'user_name'], axis=1)\n",
    "comments=comments.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE THE WHOLE GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a digraph\n",
    "B = nx.DiGraph()\n",
    "#add nodes of bipartite graph from users and videos\n",
    "B.add_nodes_from(comments['user_channel_id'], bipartite=0)\n",
    "B.add_nodes_from(comments['video_id'], bipartite=1)\n",
    "#add edges\n",
    "B.add_edges_from([(row['user_channel_id'], row['video_id']) for idx, row in comments.iterrows()])\n",
    "#set position for the plot\n",
    "pos = {node:[0, i] for i,node in enumerate(comments['user_channel_id'])}\n",
    "pos.update({node:[1, i] for i,node in enumerate(comments['video_id'])})\n",
    "\n",
    "#save outdegree of the nodes\n",
    "degs=B.out_degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE THE SUBGRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the nodes we want to study\n",
    "keep=[x for x in B.nodes() if degs[x]>146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new df where the user channels appears in the one with the conditions stated in 'keep'\n",
    "queryDF=comments.loc[comments.user_channel_id.isin(keep)]\n",
    "\n",
    "B1 = nx.DiGraph()\n",
    "#add nodes of bipartite graph from users and videos\n",
    "B1.add_nodes_from(queryDF['user_channel_id'], bipartite=0)\n",
    "B1.add_nodes_from(queryDF['video_id'], bipartite=1)\n",
    "#add edges\n",
    "B1.add_edges_from([(row['user_channel_id'], row['video_id']) for idx, row in queryDF.iterrows()])\n",
    "#set position for the plot\n",
    "pos = {node:[0, i] for i,node in enumerate(queryDF['user_channel_id'])}\n",
    "pos.update({node:[1, i] for i,node in enumerate(queryDF['video_id'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus as list of comment of every user from the net \n",
    "corpus=[]\n",
    "for user in users:\n",
    "    his_comm=comments.loc[comments.user_channel_id==user]\n",
    "    his_list=[]\n",
    "    for i in his_comm.index: \n",
    "        temp=(his_comm.comment_text_original[i])\n",
    "        his_list.append(temp)\n",
    "    corpus.append(''.join(his_list))\n",
    "    \n",
    "#save it \n",
    "\"\"\"corpusSave=pd.DataFrame(corpus)\n",
    "corpusSave.to_pickle('corpus.pkl')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN THE CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the corpus\n",
    "clean=[]\n",
    "#for each user in the corpus\n",
    "for user in range(len(corpus)):\n",
    "    his_clean=[]\n",
    "    #tokenize the text of this user and create a list of words\n",
    "    tok=nltk.RegexpTokenizer(r'\\w+').tokenize(text=corpus[user])\n",
    "    for w in range(len(tok)):\n",
    "        #for each word, make it lower\n",
    "        tok[w]=tok[w].lower()\n",
    "        #check if stopword\n",
    "        if tok[w] not in stopwords.words('english'):\n",
    "            his_clean.append(WordNetLemmatizer().lemmatize(tok[w]))\n",
    "    clean.append(his_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all cleaned text together to get to a cleaned corpus\n",
    "cleaned_corpus=[]\n",
    "for i in clean:\n",
    "    cleaned_corpus.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF AND IF SCORE, NOT USED ANYMORE.\n",
    "#IDF score\n",
    "IDF_dict={}\n",
    "for i in set([item for sublist in clean for item in sublist]):\n",
    "    IDF_dict[i]=IDF(i,clean)\n",
    "    \n",
    "#TF score\n",
    "scores={}\n",
    "for i in set(clean):\n",
    "    scores[i]=TF(i,clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save networks\n",
    "nx.write_adjlist(B1, 'B1.txt')\n",
    "nx.write_adjlist(B, 'full_net.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save compressed version \n",
    "nx.write_adjlist(B1, 'B1.gz')\n",
    "nx.write_adjlist(B, 'full_net.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save comments dataframe\n",
    "comments.to_pickle('Comments_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corp_save=pd.DataFrame(cleaned_corpus)\n",
    "clean_corp_save.to_pickle('cleaned_corpus.pkl')\n",
    "\n",
    "#save the scores, no more needed\n",
    "cleanSave=pd.DataFrame(clean)\n",
    "cleanSave.to_pickle('clean.pkl')\n",
    "\n",
    "\n",
    "scoreSave=pd.DataFrame.from_dict(scores, orient='index', columns=['count'])\n",
    "scoreSave.to_pickle('scores.pkl')\n",
    "\n",
    "scoreSave=pd.DataFrame.from_dict(IDF_dict, orient='index', columns=['count'])\n",
    "scoreSave.to_pickle('IDF_dict.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
